{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model Using Amazon Review Data\n",
    "\n",
    "Using `.json` files containing Amazon reviews of clothing and shoes, and Amazon products, I created a couple topic models to explore brand insights for Columbia Sportswear. The Amazon Product data used in this project contained about 1.5 million clothing, shoe, and jewelry products and 5.7 million reviews of those products. Of those, 4,988 products were Columbia Sportswear products having a total of 27,278 reviews. Across all 27,278 reviews, Columbia products received an average rating of 4.32 out of 5. Using this data, I was able to create a few different topic models using K-means clustering to better understand strengths and weakness according to reviewers across different product groups. \n",
    "\n",
    "First, I imported all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For Kmeans clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "# For IBM Watson\n",
    "from os.path import join, dirname\n",
    "from watson_developer_cloud import PersonalityInsightsV3\n",
    "import requests\n",
    "from watson_developer_cloud import WatsonApiException\n",
    "import time\n",
    "print('Packages Imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I had a lot of things that I was going to use multiple times, so I put many of the processes into functions. This loads the review texts and creates a set of all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that loads the texts of the review dictionary and puts the relevant info into a set\n",
    "def load_texts(reviewdata):\n",
    "    texts = set()\n",
    "    for review in reviewdata:\n",
    "        if 'reviewText' in reviewdata[review]:\n",
    "            reviewtext = reviewdata[review]['reviewText']\n",
    "            summary = reviewdata[review]['summary']\n",
    "            asin = reviewdata[review]['asin']\n",
    "\n",
    "            reviewwords = '%s %s %s' % (reviewtext, summary, asin)\n",
    "            # Add the text to a set of review data\n",
    "            texts.add(reviewwords)\n",
    "    return texts\n",
    "    print('Texts Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a k-means model and saves each topic's documents to an output folder inside the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates a topic model using K-Means and saves the information to a folder     \n",
    "def kmeans_creator(documents, review_dictionary, n_topics, brand, foldername):\n",
    "    print('Creating and Saving K-Means Topic Model')\n",
    "    # Creates a list of spanish and english stop words.\n",
    "    stop_words = stopwords.words('english')\n",
    "    spanish = stopwords.words('spanish')\n",
    "    for word in spanish:\n",
    "        stop_words.append(word)\n",
    "    stop_words.append(brand)\n",
    "    \n",
    "    # Vectorizing the data and creating a matrix of data\n",
    "    vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Assuming number of topics\n",
    "    true_k = n_topics\n",
    "    # Creating a Kmeans clustering model\n",
    "    model = KMeans(n_clusters=true_k, max_iter=10000)\n",
    "    model.fit(X)\n",
    "        \n",
    "    # Looking in the model and printing the names of the clusters and the order of centroids\n",
    "    order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Iterating the number of topics\n",
    "    print(\"Top Terms per Cluster:\")\n",
    "    for i in range(true_k):\n",
    "        # printing the top four topics\n",
    "        topic_terms = [terms[ind] for ind in order_centroids[i, :4]]\n",
    "        # Printing the cluster number and the topic terms\n",
    "        print('%d: %s' % (i, ' '.join(topic_terms)))\n",
    "\n",
    "    # Saving the topics in a folder (foldername) as txt files\n",
    "    outputfiles = {}\n",
    "    print('Creating a new directory') \n",
    "    try:\n",
    "        os.mkdir(foldername)\n",
    "        \n",
    "    except OSError:\n",
    "        print('\\nDirectory already exists. Documents added to existing folder.')\n",
    "               \n",
    "    else:\n",
    "        print('\\nSuccessfully created the directory')\n",
    "    \n",
    "    for topic in range(true_k):\n",
    "        topic_terms = [terms[ind] for ind in order_centroids[topic, :4]]\n",
    "        # Creating output file inside of dictionary\n",
    "        outputfiles[topic] = open(os.path.join(foldername, '_'.join(topic_terms) + '.txt'), 'w')\n",
    "        \n",
    "    print('Filling Directory')\n",
    "    for review in review_dictionary:\n",
    "        # If there's text in this review, do something\n",
    "        if 'reviewText' in review_dictionary[review]:\n",
    "            review = review_dictionary[review]\n",
    "            reviewbit = '%s %s %s %s' % (review['asin'], review['overall'],  review['summary'], review['reviewText'])\n",
    "            # Puting Review text chunks with asins. Then find which cluster it belongs in\n",
    "                # Takes 1 review at a time, using the existing vectorizer that we've already used\n",
    "            Y = vectorizer.transform([reviewbit])\n",
    "            # Each document gets a score of how much it belongs in a certain topic.\n",
    "                # (true_k) scores per document \n",
    "            for prediction in model.predict(Y):\n",
    "                outputfiles[prediction].write('%s\\n' % (reviewbit))\n",
    "    # n = count, f = name\n",
    "    for n, f in outputfiles.items():\n",
    "        f.close()\n",
    "    print('K-Means Creation Finished and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gathers a score for all the reviews of the company in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gives an average score of a review dictionary\n",
    "def score_getter(review_dictionary):\n",
    "    score = 0\n",
    "    for i in review_dictionary:\n",
    "        score = score + review_dictionary[i]['overall']\n",
    "    print('The average score is ' + str(round((score/len(review_dictionary)), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then reformatted my IBM Watson code used in my Russian Troll Ad research to work for these reviews, gathering personality insights of the ads. This also computes the average star rating for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IBM_Watson_Cluster_Personality(user, passw, foldername, savefile):\n",
    "    # IBM Watson API------------------------------------------------\n",
    "    class WatsonException(Exception):\n",
    "        \"\"\"\n",
    "        Custom exception class for Watson Services.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    class WatsonApiException(WatsonException):\n",
    "        \"\"\"\n",
    "        Custom exception class for errors returned from Watson APIs.\n",
    "    \n",
    "        :param int code: The HTTP status code returned.\n",
    "        :param str message: A message describing the error.\n",
    "        :param dict info: A dictionary of additional information about the error.\n",
    "        \"\"\"\n",
    "        def __init__(self, code, message, info=None):\n",
    "            # Call the base class constructor with the parameters it needs\n",
    "            super(WatsonApiException, self).__init__(message)\n",
    "            self.message = message\n",
    "            self.code = code\n",
    "            self.info = info\n",
    "    \n",
    "        def __str__(self):\n",
    "            return 'Error: ' + self.message + ', Code: ' + str(self.code)\n",
    "    # Gathering data from folder and saving the names and the contents to separate lists\n",
    "    filelst = []\n",
    "    namelst = []\n",
    "    for filename in os.listdir(foldername):\n",
    "        with open(os.path.join(foldername, filename)) as f:\n",
    "            contentlst = []\n",
    "            namelst.append(filename)\n",
    "            for line in f:\n",
    "                contentlst.append(line)\n",
    "            filelst.append(contentlst)\n",
    "    # Authentification info for IBM Watson\n",
    "    service = PersonalityInsightsV3(\n",
    "        version='2017-10-13',\n",
    "        ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
    "        # url='https://gateway.watsonplatform.net/personality-insights/api',\n",
    "        username=user,\n",
    "        password=passw)\n",
    "    \n",
    "    # Test: Asking the watson to analyze the inputted string.\n",
    "    response = service.profile(\n",
    "        'YOUR TEXT HERE the dog and the dog the dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dogthe dog and the dog', # Must be 100 words in length\n",
    "        content_type='text/plain',\n",
    "        accept=\"text/csv\",\n",
    "        charset='utf-8',\n",
    "        csv_headers=True).get_result()\n",
    "    \n",
    "    #print(response.content)\n",
    "    # Splitting the lines from the headers and the variables\n",
    "    profile = response.content\n",
    "    cr = profile.splitlines()\n",
    "    \n",
    "    # Creating column headers\n",
    "    labelslst = []\n",
    "    # The data is in one long list of bytes. We need to convert this to strings\n",
    "    letter = ''\n",
    "    # Iterating over each set of bytes in the list\n",
    "    # This little for loops creates a column of soon-to-be column headers \n",
    "        # from the bytes gathered by test call to Watson\n",
    "    for i in cr[0]:\n",
    "        # The letter = the character value converted from ASCII decimal\n",
    "        letter = letter + chr(i)\n",
    "        # If the byte is 44 (a comma), append the full letter value to the labelslst\n",
    "        if i == 44:\n",
    "            letter = letter[:-1]\n",
    "            labelslst.append(letter)\n",
    "            #print(letter)\n",
    "            letter = ''\n",
    "    # Create a dataframe of the labels\n",
    "    insightsdf = pd.DataFrame(labelslst)\n",
    "            \n",
    "     # For each ad in the textlst,\n",
    "    for item in range(len(filelst)):\n",
    "        itemtxt = ''.join(filelst[item])\n",
    "        try:\n",
    "            # API call to Watson\n",
    "            response = service.profile(\n",
    "                    itemtxt,\n",
    "                    content_type='text/plain',\n",
    "                    accept=\"text/csv\",\n",
    "                    charset='utf-8',\n",
    "                    csv_headers=True).get_result()\n",
    "            \n",
    "            profile = response.content\n",
    "            cr = profile.splitlines()\n",
    "            \n",
    "            # Appending values for the text to a list\n",
    "            vallst = []\n",
    "            val = ''\n",
    "            for i in cr[1]:\n",
    "                # The letter = the character value converted from ASCII decimal\n",
    "                val = val + chr(i)\n",
    "                # If the byte is 44 (a comma), append the full letter value to the labelslst\n",
    "                if i == 44:\n",
    "                    try:\n",
    "                        val = val[:-1]\n",
    "                        val = float(val)\n",
    "                        vallst.append(val)        \n",
    "                    except ValueError:\n",
    "                        vallst.append(1)\n",
    "                    val = ''\n",
    "            # Appending the list to the dataframe, leaving room for the column headers\n",
    "            insightsdf[item+1] = vallst\n",
    "            time.sleep(-time.time()%1)\n",
    "        except WatsonApiException:\n",
    "            insightsdf[item+1] = 0\n",
    "            continue \n",
    "        print(str(item) + ' Done!')  \n",
    "\n",
    "    # Transpose the data frame \n",
    "    insightsdfT = insightsdf.T \n",
    "    # Add column names as the labels\n",
    "    insightsdfT.columns = insightsdfT.iloc[0]\n",
    "    # Will need to drop the 0th row once we have data inside the dataframe\n",
    "    insightsdfT = insightsdfT.iloc[1:]\n",
    "    insightsdfT['Clusters'] = namelst\n",
    "    \n",
    "    # Adding the average scores to the data frame\n",
    "    kmeansdict = {}\n",
    "    for filename in os.listdir(foldername):\n",
    "        with open(os.path.join(foldername, filename)) as f:\n",
    "            ratinglst = []\n",
    "            kmeansdict[filename] = []\n",
    "            for line in f:\n",
    "                ratinglst.append(float(line[11:14]))\n",
    "            kmeansdict[filename].append(ratinglst)\n",
    "    \n",
    "    # Printing average score of each cluster\n",
    "    avgscore = []\n",
    "    for cluster in kmeansdict:\n",
    "         print(cluster + ': ')\n",
    "         print(sum(kmeansdict[cluster][0])/len(kmeansdict[cluster][0]))\n",
    "         print('\\n')\n",
    "         avgscore.append(sum(kmeansdict[cluster][0])/len(kmeansdict[cluster][0]))\n",
    "    insightsdfT['Avg Scores'] = avgscore\n",
    "    \n",
    "    insightsdfT.to_csv(savefile)\n",
    "    print('IBM Watson Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're able to create a dictionary of product information. Though this file contains over 1.5 million products, it runs relatively quickly despite the nested for-loops. To know where the iteration is at, it will display a count every 100,000 product entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Product Dictionary\n",
      "100,000 Items Created\n",
      "200,000 Items Created\n",
      "300,000 Items Created\n",
      "400,000 Items Created\n",
      "500,000 Items Created\n",
      "600,000 Items Created\n",
      "700,000 Items Created\n",
      "800,000 Items Created\n",
      "900,000 Items Created\n",
      "1,000,000 Items Created\n",
      "1,100,000 Items Created\n",
      "1,200,000 Items Created\n",
      "1,300,000 Items Created\n",
      "1,400,000 Items Created\n",
      "1,500,000 Items Created\n",
      "Product Dictionary and Category List Created\n"
     ]
    }
   ],
   "source": [
    "# Loading in the json file of products\n",
    "loadedjson = open('meta_Clothing_Shoes_and_Jewelry.json', 'r')\n",
    "print('Creating Product Dictionary')\n",
    "count = 0    \n",
    "allproducts = {}\n",
    "catlist = {}\n",
    "# Iterating over each entry\n",
    "for row in loadedjson:\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(str(format(count, ',')) + ' Items Created')\n",
    "    # Putting the product info back into a dictionary from a printed output\n",
    "    product = eval(row)\n",
    "\n",
    "    # Keeping the ASIN number for each product\n",
    "    allproducts[product['asin']] = product\n",
    "    # Iterating over each category in each product\n",
    "    for categories in product['categories']:\n",
    "\n",
    "        # Adding categories to the list\n",
    "        for acategory in categories:\n",
    "            if acategory in catlist:\n",
    "                catlist[acategory] += 1\n",
    "            else:\n",
    "                catlist[acategory] = 1\n",
    "print('Product Dictionary and Category List Created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to look at Columbia Sportswear as a brand, so I created a set of Columbia Amazon Standard Identification Numbers, or ASINs for short. This will allow us to look through the `.json` full of product reviews and match review to product ASIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Set of ASINs\n",
      "6.65%\n",
      "13.3%\n",
      "19.95%\n",
      "26.61%\n",
      "33.26%\n",
      "39.91%\n",
      "46.56%\n",
      "53.21%\n",
      "59.86%\n",
      "66.52%\n",
      "73.17%\n",
      "79.82%\n",
      "86.47%\n",
      "93.12%\n",
      "99.77%\n",
      "ASIN Set Created\n"
     ]
    }
   ],
   "source": [
    "# Writing the Columbia asins to a txt file\n",
    "count = 0\n",
    "allcolumbiaasins = set() # Set is just a unique list\n",
    "print('Creating Set of ASINs')  \n",
    "for product in allproducts: # Iterating through the products in all the products\n",
    "    theproduct = allproducts[product] # setting a variable equal to the current product\n",
    "    \n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(str(round(100*(count/len(allproducts)),2)) + '%')\n",
    "        \n",
    "    # iterating through the categories that the product is in\n",
    "    for categories in theproduct['categories']: \n",
    "        # Iterating through each category in each list\n",
    "        for acategory in categories:\n",
    "            # If columbia is a category, add the asin to the set\n",
    "            if 'columbia' in acategory.lower():\n",
    "                allcolumbiaasins.add(theproduct['asin'])\n",
    "print('ASIN Set Created')\n",
    "with open(\"columbia.txt\", \"w\") as output:\n",
    "    output.write(str(allcolumbiaasins))  \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains over 5.7 million reviews for products in the clothing, shoes and jewelry section of amazon. When finished, our dictionary will contain review text, review title, reviewer ID, star rating, and the product ASIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Review Dictionary\n",
      "1.75%\n",
      "3.51%\n",
      "5.26%\n",
      "7.02%\n",
      "8.77%\n",
      "10.53%\n",
      "12.28%\n",
      "14.04%\n",
      "15.79%\n",
      "17.54%\n",
      "19.3%\n",
      "21.05%\n",
      "22.81%\n",
      "24.56%\n",
      "26.32%\n",
      "28.07%\n",
      "29.82%\n",
      "31.58%\n",
      "33.33%\n",
      "35.09%\n",
      "36.84%\n",
      "38.6%\n",
      "40.35%\n",
      "42.11%\n",
      "43.86%\n",
      "45.61%\n",
      "47.37%\n",
      "49.12%\n",
      "50.88%\n",
      "52.63%\n",
      "54.39%\n",
      "56.14%\n",
      "57.89%\n",
      "59.65%\n",
      "61.4%\n",
      "63.16%\n",
      "64.91%\n",
      "66.67%\n",
      "68.42%\n",
      "70.18%\n",
      "71.93%\n",
      "73.68%\n",
      "75.44%\n",
      "77.19%\n",
      "78.95%\n",
      "80.7%\n",
      "82.46%\n",
      "84.21%\n",
      "85.96%\n",
      "87.72%\n",
      "89.47%\n",
      "91.23%\n",
      "92.98%\n",
      "94.74%\n",
      "96.49%\n",
      "98.25%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Creating Review Dictionary')\n",
    "# Reading in the json for reviews\n",
    "loadjson2 = open('reviews_Clothing_Shoes_and_Jewelry.json', 'r')\n",
    "allreviews = {}\n",
    "count = 0\n",
    "for line in loadjson2:\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(str(round(100*(count/5700000),2)) + '%')\n",
    "        \n",
    "    review = eval(line)\n",
    "    theasin = review['asin']\n",
    "    \n",
    "    if theasin in allcolumbiaasins:\n",
    "        thekey = '%s.%s' % (theasin, review['reviewerID'])\n",
    "        allreviews[thekey] = review\n",
    "json.dump(allreviews, open('ColumbiaReviews.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can load the reviews and create a k-means cluster analysis with them. The topics will be saved to an output folder in our working directory. I've specified 9 clusters, as this was the number where most of the duplicate clusters started to fade out. With this method of k-means, it's important to try to narrow down the number of clusters to avoid duplicate clusters, while still having as many meaningful clusters as we can. These nine clusters are: `Boots`, `Quality and Pricing`, `Warmth and Fit`, `Jackets`, `Gifts`, `Shoes`, `Sizing`, `Vests`, and `Wallets`. These nine clusters represent a wide variety of products and with the slight exception of Warmth and Fit, they seem to be fairly distinct in their topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and Saving K-Means Topic Model\n",
      "Top Terms per Cluster:\n",
      "0: great good warm fit\n",
      "1: vest b003nx8c2o b00062nnlk great\n",
      "2: boot boots warm great\n",
      "3: jacket great warm nice\n",
      "4: pants great fit b003s9vuh2\n",
      "5: shoes shoe comfortable great\n",
      "6: coat great warm nice\n",
      "7: size small large ordered\n",
      "8: boots warm feet snow\n",
      "Creating a new directory\n",
      "\n",
      "Directory already exists. Documents added to existing folder.\n",
      "Filling Directory\n",
      "K-Means Creation Finished and Saved\n",
      "The average score is 4.32\n"
     ]
    }
   ],
   "source": [
    "documents = list(load_texts(allreviews))\n",
    "kmeans_creator(documents, allreviews, 9, 'columbia', 'output')\n",
    "score_getter(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of cluster names and cluster contents            \n",
    "print('Analyzing Review Dictionary with IBM Watson')\n",
    "IBM_Watson_Cluster_Personality('USERNAME','PASSWORD', 'output', 'cluster_personality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking through my clusters, I noticed that `wallet_cards_pocket_leather` and `size_small_large_ordered` had the lowest score out of any the clusters, with `size_small_large_ordered` being significantly lower than any other clusters. I decided to subset these and do further analysis on these specific clusters, looking at personality insights for each.\n",
    "\n",
    "The ‘Adjusted Wallet’ cluster had 66 ratings below 3.0 stars (not inclusive) with an average rating of 1.530 and the ‘Adjusted Sizing’ cluster had 433 ratings below 3.0 stars (not inclusive) with an average rating of 1.643. This is a significantly higher proportion of poor reviews in the ‘Adjusted Sizing’ cluster than the ‘Adjusted Wallet’ cluster.  Simply reading through the reviews in the ‘Adjusted Sizing’ cluster, one gets an idea of the problem in this cluster: the sizing is unexpected by customers. Many customers state that the product is smaller than what they expect and that the products “run small”. In fact, the word “small” shows up in over half of the ‘Adjusted Sizing’ cluster reviews that have a rating below 3.0 (not inclusive), or 280 times in just 433 reviews. Other reviewers state that the color or the thickness is not what they expect – most likely a common problem across all clothing sales online. Reading through the reviews in the ‘Adjusted Wallet’ cluster, one gets a sense that customers see the products within as ‘poor quality’ and that the wallet is not durable. Two reviews state that the wallets smell bad.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory already exists.\n",
      "Number of times \"small\" is mentioned:  278\n"
     ]
    }
   ],
   "source": [
    "# Low scoring areas: Wallets and Sizing\n",
    "poorproductlst = set()\n",
    "for filename in os.listdir('output'):\n",
    "    if filename == 'wallet_cards_pocket_leather.txt':\n",
    "        with open(os.path.join('output', filename)) as f:\n",
    "            walletlst = []\n",
    "            for line in f:\n",
    "                if float(line[11:13]) < 3.0:\n",
    "                    walletlst.append(line)\n",
    "                    poorproductlst.add(line[0:10])\n",
    "    elif filename == 'size_small_large_ordered.txt':\n",
    "        with open(os.path.join('output', filename)) as f:\n",
    "            sizelst = []\n",
    "            for line in f:\n",
    "                if float(line[11:13]) < 3.0:\n",
    "                    sizelst.append(line)\n",
    "                    poorproductlst.add(line[0:10])\n",
    "# Saving the files to their own folder\n",
    "try:\n",
    "    os.mkdir('poor_clusters') \n",
    "except OSError:\n",
    "    print('\\nDirectory already exists.')     \n",
    "else:\n",
    "    print('\\nSuccessfully created the directory')\n",
    "with open('poor_clusters\\walletbadreviews.txt', 'w') as output:\n",
    "    for item in walletlst:\n",
    "        output.write(\"%s\" % item)\n",
    "with open(\"poor_clusters\\sizebadreviews.txt\", \"w\") as output:\n",
    "    for item in sizelst:\n",
    "        output.write('%s' % item)  \n",
    "output.close()\n",
    "\n",
    "# Finding insights based on word count\n",
    "count = 0\n",
    "for i in sizelst:\n",
    "    txt = i.lower()\n",
    "    if 'small' in txt:\n",
    "        count += 1\n",
    "print('Number of times \"small\" is mentioned: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running IBM Watson on the two files\n",
    "IBM_Watson_Cluster_Personality('USERNAME','PASSWORD', 'poor_clusters', 'poorcluster_personality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I wanted to subset the reviews based on rating. With the code below, I created two more review dictionaries - one for reviews equal to or over 4.0 and one for reviews equal to or lower than 2.0. This would allow me to see what are common topics in each to better make branding decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating High and Low review dictionaries based on star rating\n",
    "highreviews = {}\n",
    "lowreviews = {}\n",
    "count = 0\n",
    "loadjson2 = open('reviews_Clothing_Shoes_and_Jewelry.json', 'r')\n",
    "for line in loadjson2:\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(str(format(count, ',')) + ' Items Created')\n",
    "        \n",
    "    review = eval(line)\n",
    "    theasin = review['asin']\n",
    "    stars = review['overall']\n",
    "    stars = float(stars)\n",
    "    \n",
    "    if stars >= 4.0:\n",
    "        if theasin in allcolumbiaasins:\n",
    "            thekey = '%s.%s' % (theasin, review['reviewerID'])\n",
    "            highreviews[thekey] = review\n",
    "    if stars <= 2.0:\n",
    "        if theasin in allcolumbiaasins:\n",
    "            thekey = '%s.%s' % (theasin, review['reviewerID'])\n",
    "            lowreviews[thekey] = review\n",
    "print('High and Low Review Dictionaries Created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying IBM Watson Personality Insights to rating-sifted dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering the high and low reviews separately and applying IBM Watson API\n",
    "documents = list(load_texts(highreviews))\n",
    "kmeans_creator(documents, highreviews, 5, 'columbia', 'outputHigh')\n",
    "score_getter(highreviews)\n",
    "json.dump(highreviews, open('ColumbiaHighReviews.json', 'w'))\n",
    "print('Analyzing High Review Dictionary with IBM Watson')\n",
    "IBM_Watson_Cluster_Personality('USERNAME','PASSWORD', 'outputHigh', 'Highcluster_personality.csv')\n",
    "\n",
    "documents = list(load_texts(lowreviews))\n",
    "kmeans_creator(documents, lowreviews, 5, 'columbia', 'outputLow')\n",
    "score_getter(lowreviews)\n",
    "json.dump(lowreviews, open('ColumbiaLowReviews.json', 'w'))\n",
    "print('Analyzing Low Review Dictionary with IBM Watson')\n",
    "IBM_Watson_Cluster_Personality('USERNAME','PASSWORD', 'outputLow', 'Lowcluster_personality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
