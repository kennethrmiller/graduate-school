{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras neural net to predict clicks on Russian Facebook and Instagram ads.\n",
    "Unfortunately, this did not predict very well and still needs some work on the layering and optimization portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This are some keras imports;\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer # Import the tokenizer to split the texts;\n",
    "from keras.models import Sequential # Keras sequential approach to neural nets;\n",
    "from keras.optimizers import Adam # Keras optimizer\n",
    "from keras.layers import Dense # Some neural network layers;\n",
    "from keras import backend as K # Allows us to calculate R-squared\n",
    "from keras import layers\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                107600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 112,751\n",
      "Trainable params: 112,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1264 samples, validate on 543 samples\n",
      "Epoch 1/5\n",
      "1264/1264 [==============================] - 0s 351us/step - loss: 9292278.0000 - coeff_determination: -0.2803 - val_loss: 10722482.0000 - val_coeff_determination: -0.2010\n",
      "Epoch 2/5\n",
      "1264/1264 [==============================] - 0s 54us/step - loss: 9292278.0000 - coeff_determination: -0.2803 - val_loss: 10722482.0000 - val_coeff_determination: -0.2010\n",
      "Epoch 3/5\n",
      "1264/1264 [==============================] - 0s 54us/step - loss: 9292278.0000 - coeff_determination: -0.2803 - val_loss: 10722482.0000 - val_coeff_determination: -0.2010\n",
      "Epoch 4/5\n",
      "1264/1264 [==============================] - 0s 54us/step - loss: 9292278.0000 - coeff_determination: -0.2803 - val_loss: 10722482.0000 - val_coeff_determination: -0.2010\n",
      "Epoch 5/5\n",
      "1264/1264 [==============================] - 0s 57us/step - loss: 9292277.0000 - coeff_determination: -0.2803 - val_loss: 10722482.0000 - val_coeff_determination: -0.2010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1afa6bdda20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras Neuralnet (did not predict)-----------------------------------------------\n",
    "neuraldf = pd.read_csv('russiadata-bestmodel.csv')\n",
    "neuraldf = pd.get_dummies(neuraldf, columns = ['Ad Landing Page '])\n",
    "# neuraldf.to_csv('Neural Cleaned No nulls.csv')\n",
    "\n",
    "text_data = neuraldf[[\"Ad Text \"]].values.reshape(1,-1)[0] # have to use ndarray to mine text\n",
    "tokenizer = Tokenizer(num_words=1000) # How to split the text... basically dummy codes each word\n",
    "# 1000 words is a good amount... the more words I feed, the less accurate it gets\n",
    "# (Tokenizer might also accept a list)\n",
    "\n",
    "#figuring out what our 1000 words are\n",
    "tokenizer.fit_on_texts(text_data) #Fitting data, gives each word a unique id\n",
    "\n",
    "#creating a numpy arrary with the tokenized matrix for each post\n",
    "#(aka actually applying our tokenizer to each piece of text)\n",
    "wordmatrix = tokenizer.texts_to_matrix(text_data)\n",
    "otherpredictors = neuraldf.drop('Ad Text ', axis=1) # Dropping this\n",
    "\n",
    "#converting predictors to numpy array, the required format\n",
    "#sucks, but we lose the variable names here\n",
    "otherpredictorsmatrix = otherpredictors.values\n",
    "\n",
    "#just adding all of our predictors together\n",
    "# Concatenating the matrix of text and other predictors\n",
    "X = np.concatenate((wordmatrix, otherpredictorsmatrix), axis=1) \n",
    "#this is the variable we're goint to try and predict\n",
    "y = neuraldf[[\"Ad Clicks \"]] #Getting the label as y;\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "#first, the Keras model needs to know the number of columns in the model\n",
    "dimensions = X.shape[1]\n",
    "\n",
    "\n",
    "#adding our first layer\n",
    "model.add(Dense(50,input_dim=dimensions,activation=\"relu\")) # * Input layer *\n",
    "\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation=\"softmax\")) # * Output Layer *\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "#two key parameters here learning rate == lr the higher lr is, the more shortcuts it takes\n",
    "#and the name of the optimizer itself\n",
    "#https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2\n",
    "optimizer = Adam(lr=.0001) #Preparing optimizer;\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "#Actually getting the model all ready to run\n",
    "model.compile(loss=\"mean_squared_error\", optimizer= optimizer, metrics=[coeff_determination]) \n",
    "\n",
    "#running the model\n",
    "model.fit(X, y, batch_size=X.shape[0], epochs=5, shuffle=True, validation_split=0.3)#Training data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
