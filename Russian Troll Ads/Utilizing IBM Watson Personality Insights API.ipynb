{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Watson Personality Insights API\n",
    "## Kenneth R Miller\n",
    "### This is code written to gather personality insights for a dataset of Russian Troll Facebook and Instagram ads. It iterates through each ad and gains insight for each ad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# For the Personality Insights module of Watson\n",
    "from watson_developer_cloud import PersonalityInsightsV3\n",
    "# To ignore error warnings so that the code does not break\n",
    "from watson_developer_cloud import WatsonApiException\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to ignore API Exceptions. This is unecessary if your every bit of text is at least 100 words long and contains 70 or more recognizeable words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should allow us to ignore the Watson API exception warnings.\n",
    "# WatsonAPIException is usually thrown when text is less than 100 words \n",
    "    # or contains less than 70 recognizeable words.\n",
    "class WatsonException(Exception):\n",
    "    \"\"\"\n",
    "    Custom exception class for Watson Services.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class WatsonApiException(WatsonException):\n",
    "    \"\"\"\n",
    "    Custom exception class for errors returned from Watson APIs.\n",
    "\n",
    "    :param int code: The HTTP status code returned.\n",
    "    :param str message: A message describing the error.\n",
    "    :param dict info: A dictionary of additional information about the error.\n",
    "    \"\"\"\n",
    "    def __init__(self, code, message, info=None):\n",
    "        # Call the base class constructor with the parameters it needs\n",
    "        super(WatsonApiException, self).__init__(message)\n",
    "        self.message = message\n",
    "        self.code = code\n",
    "        self.info = info\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Error: ' + self.message + ', Code: ' + str(self.code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('russiadata.minusunknowns.csv')\n",
    "# Dropping rows with NA values\n",
    "data = data.dropna(axis = 0, how = 'any')\n",
    "# Getting rid of all the non-character values in the text\n",
    "data['Ad Text '] = data['Ad Text '].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentification information schema for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentification info for IBM Watson\n",
    "service = PersonalityInsightsV3(\n",
    "    version='2017-10-13',\n",
    "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
    "    # url='https://gateway.watsonplatform.net/personality-insights/api',\n",
    "    username='USERNAME',\n",
    "    password='PASSWORD')\n",
    "\n",
    "# Test: Asking the watson to analyze the inputted string.\n",
    "response = service.profile(\n",
    "    'YOUR TEXT HERE', # Must be 100 words in length\n",
    "    content_type='text/plain',\n",
    "    accept=\"text/csv\",\n",
    "    charset='utf-8',\n",
    "    csv_headers=True).get_result()\n",
    "\n",
    "print(response.content)\n",
    "# Splitting the lines from the headers and the variables\n",
    "profile = response.content\n",
    "cr = profile.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe of return labels, later to be merged with text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelslst = []\n",
    "# The data is in one long list of bytes. We need to convert this to strings\n",
    "letter = ''\n",
    "# Iterating over each set of bytes in the list\n",
    "# This little for loops creates a column of soon-to-be column headers \n",
    "    # from the bytes gathered by test call to Watson\n",
    "for i in cr[0]:\n",
    "    # The letter = the character value converted from ASCII decimal\n",
    "    letter = letter + chr(i)\n",
    "    # If the byte is 44 (a comma), append the full letter value to the labelslst\n",
    "    if i == 44:\n",
    "        letter = letter[:-1]\n",
    "        labelslst.append(letter)\n",
    "        print(letter)\n",
    "        letter = ''\n",
    "# Create a dataframe of the labels\n",
    "personalitydf = pd.DataFrame(labelslst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling Watson API for personality insights for each segment of text in the dataframe. It would be easy to have speed this code up by moving the byte-to-integer calculation outside of the call, but I like this self contained model. Additionally, it must be slightly slow so that the API can keep up (hence the sleep function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all the clean ad text\n",
    "textlst = data['Ad Text ']\n",
    "\n",
    "# Iterating over each segment of text\n",
    "for ad in range(len(texlst)):\n",
    "    adtext = textlst[ad]\n",
    "    # If the number of words is >= 100, call\n",
    "    if len(adtext.split()) >= 100:\n",
    "        try:\n",
    "            # API call to Watson\n",
    "            response = service.profile(\n",
    "                    adtext,\n",
    "                    content_type='text/plain',\n",
    "                    accept=\"text/csv\",\n",
    "                    charset='utf-8',\n",
    "                    csv_headers=True).get_result()\n",
    "            \n",
    "            profile = response.content\n",
    "            cr = profile.splitlines()\n",
    "            \n",
    "            # Appending values for the text to a list\n",
    "            vallst = []\n",
    "            val = ''\n",
    "            # cr[1] is the values line, not the label line. We obtained labels above\n",
    "            for i in cr[1]:\n",
    "                # The value = the character value converted from ASCII decimal\n",
    "                val = val + chr(i)\n",
    "                # If the byte is 44 (a comma), append the full value to the labelslst\n",
    "                if i == 44:\n",
    "                    try:\n",
    "                        val = val[:-1]\n",
    "                        val = float(val)\n",
    "                        vallst.append(val)        \n",
    "                    except ValueError:\n",
    "                        # One column returns a language code. I've set this up to put a 1 in this column, but this is easily edited\n",
    "                        vallst.append(1)\n",
    "                    val = ''\n",
    "            # Appending the list to the dataframe, leaving room for the column headers\n",
    "            personalitydf[ad+1] = vallst\n",
    "            time.sleep(-time.time()%1)\n",
    "        except WatsonApiException:\n",
    "            # Putting NA in for text values that are too short or incomrehensible by Watson\n",
    "            personalitydf[ad+1] = NA\n",
    "            continue \n",
    "    # Else, input 0 for personality data\n",
    "    else:\n",
    "        personalitydf[ad+1] = 0\n",
    "    # Allows us to keep track of where we are in the dataframe\n",
    "    print(str(ad) + ' Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose the data and merge it with the original dataset. Now we have a the personality insights merged with the original ads, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the data frame from 2600 column x 138 rows to 138 columns x 2600 rows\n",
    "personalitydfT = personalitydf.T \n",
    "# Add column names as the labels\n",
    "personalitydfT.columns = personalitydfT.iloc[0]\n",
    "# Will need to drop the 0th row once we have data inside the dataframe\n",
    "personalitydfT = personalitydfT.iloc[1:]\n",
    "# Merge the IBM Watson Personality Insights with the original data\n",
    "result = pd.concat([data, personalitydfT], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
